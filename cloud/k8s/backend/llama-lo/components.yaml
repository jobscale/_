apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-lo
  labels:
    app: llama-lo
    component: deployment
spec:
  selector:
    matchLabels:
      app: llama-lo
  template:
    metadata:
      labels:
        app: llama-lo
        component: workload
    spec:
      containers:
      - image: ghcr.io/jobscale/reverse-proxy
        name: llama-lo
        env:
        - name: BACKEND
          value: http://llama.x.jsx.jp:2880
---
apiVersion: v1
kind: Service
metadata:
  name: llama-lo
  labels:
    app: llama-lo
    component: service
spec:
  selector:
    app: llama-lo
  type: ClusterIP
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 3000
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: llama-lo
  labels:
    name: llama-lo
  annotations:
    nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"
spec:
  ingressClassName: nginx
  tls:
  - secretName: jsxjp-tls
    hosts: [ "llama.jsx.jp", "llama.x.jsx.jp" ]
  rules:
  - host: llama.jsx.jp
    http:
      paths:
      - pathType: Prefix
        path: /
        backend:
          service:
            name: llama-lo
            port:
              number: 80
  - host: llama.x.jsx.jp
    http:
      paths:
      - pathType: Prefix
        path: /
        backend:
          service:
            name: llama-lo
            port:
              number: 80
