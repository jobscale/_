#!/usr/bin/env bash

IMAGE="kindest/node:v1.34.0"

production() {
  export CLUSTER_NAME=production
  export CLUSTER_CONFIG=kindest/production.yaml
}
staging() {
  export CLUSTER_NAME=staging
  export CLUSTER_CONFIG=kindest/staging.yaml
}
development() {
  export CLUSTER_NAME=development
  export CLUSTER_CONFIG=kindest/development.yaml
}

development

k8sVolume() {
  MNT_DIR=/home/k8s/mnt/var/lib
  sudo mkdir -p $MNT_DIR/_empty
  sudo bash -c "chown root:adm $MNT_DIR/*"
  sudo bash -c "chmod g+rX $MNT_DIR/*"
  sudo tree -h -L 2 $MNT_DIR
}

allInOne() {
  k8sVolume
  echo -n "setup '$CLUSTER_NAME' deployment "
  read -p "ok? (y/N): " yn; case "$yn" in [yY]*) echo "continue";; *) echo "abort" && return;; esac

  setupWatches
  cleanKind
  LOGFILE=/home/k8s/logs/kind-$CLUSTER_NAME-$(date +'%Y-%m-%d-%H:%M:%S').log
  time inAllInOne 2>&1 | tee $LOGFILE
}

setupWatches() {
  . /etc/os-release
  if [[ $VERSION_CODENAME == trixie ]]; then
    if [[ ! -s /etc/sysctl.d/99-inotify.conf ]]; then
      echo 'Setup "/etc/sysctl.d/99-inotify.conf" fs.inotify.max_user 512K'
      cat <<EOF | sudo tee /etc/sysctl.d/99-inotify.conf
fs.inotify.max_user_watches=524288
fs.inotify.max_user_instances=8192
EOF
      sudo chmod go+r /etc/sysctl.d/99-inotify.conf
      sudo systemctl restart systemd-sysctl.service
      echo
    fi
  elif [[ $(sudo grep fs.inotify.max_user /etc/sysctl.conf | wc -l) -lt 2 ]]; then
    echo 'Setup "/etc/sysctl.conf" fs.inotify.max_user 512K'
    cat <<EOF | sudo tee -a /etc/sysctl.conf
fs.inotify.max_user_watches=524288
fs.inotify.max_user_instances=8192
EOF
    sudo chmod go+r /etc/sysctl.conf
    sudo sysctl -p
    echo
  fi
}

cleanKind() {
  sudo rm -fr /home/k8s/system
  sudo rm -fr /home/k8s/mnt
  sudo mkdir -p /home/k8s/logs
  sudo chown -R $(id -un):$(id -gn) /home/k8s
}

initNetwork() {
  sudo ln -sfn /usr/lib/systemd/resolv.conf /etc/resolv.conf
  echo "nameserver 8.8.8.8" | sudo tee /etc/resolv.conf
  sudo systemctl disable systemd-resolved --now
  sudo systemctl mask systemd-resolved
}

inAllInOne() {
  # initNetwork

  createKind

  kubectl -n default apply -f backend/limitrange-limits.yaml
  ingressTlsSecret default

  # pullImages
  defaultImage

  k8sApps
  # partnerContainer
  createJsx
  createMqtt
  createReverse
  createNetworkApps
  [[ $CLUSTER_NAME == production ]] && createCronJob

  # createDatabase
  # createStandard
  # [[ $CLUSTER_NAME == production ]] && createCodeServer
  # [[ $CLUSTER_NAME == production ]] && createWetty
  # [[ $CLUSTER_NAME == production ]] && deployWordpress
  # [[ $CLUSTER_NAME == production ]] && deployBitnami
  # [[ $CLUSTER_NAME == production ]] && deployRedmine

  # metalLB
  # ingressForward
  # ingressNginxSecure
  ingressNginxSimple
  loop.js 20 --silent
  # ingressRestart

  echo -n "        ending" && loop.js 20 --silent

  kubectl get pods -A -o wide
  kubectl top pods -n standard --containers | sort -h -k 4
  docker exec -i $CLUSTER_NAME-control-plane crictl images | sort > images-$CLUSTER_NAME-list
  echo -e '\n\nk8s creation finished.\n\n'

  ingressTlsSecret kubernetes-dashboard
  loop.js 20 --silent
  echo "kubectl -n kubernetes-dashboard apply -f backend/k8s"
  loop.js 20 --silent
  kubectl -n kubernetes-dashboard apply -f backend/k8s
  loop.js 20 --silent

  # Allow API
  # ingressNginxAllow

  token && postSlack "url: https://k8s.jsx.jp token: $(token)"
  echo Completed.
}

check-wp() {
  kubectl exec -n wetty -it svc/wetty -- bash -c "curl -I -s wordpress.wordpress.svc.cluster.local:80"
  kubectl exec -n wetty -it svc/wetty -- bash -c "curl -I -s wp-bitnami.bitnami.svc.cluster.local:80"
}

createKind() {
  DATE=$(date +'%Y-%m-%d_%H:%M:%S')
  [[ -s $HOME/.kube/config ]] && mv $HOME/.kube/config $HOME/.kube/save-config_$DATE
  docker pull "$IMAGE"
  kind create cluster --name $CLUSTER_NAME --config $CLUSTER_CONFIG --image "$IMAGE"
  cp $HOME/.kube/config $HOME/.kube/kind-config-$CLUSTER_NAME-$DATE
  sed -i -e 's/0\.0\.0\.0/kubernetes/' $HOME/.kube/config

  echo -n "        wait for nodes system pods initializing"
  loop.js 10 --silent
  for i in {1..20}
  do
    loop.js 5 --silent
    [[ $(kubectl get all -A | grep -e 0/1 | wc -l) == 0 ]] && break
    kubectl get all -A | grep -e 0/1 || echo
  done
}

metricsServer() {
  METRICS_SERVER_VERSION=$(git ls-remote --refs --tags https://github.com/kubernetes-sigs/metrics-server.git | sort -t '/' -k 3 -V | tail -1 | awk -F/ '{print $3}')
  echo "kubernetes-sigs metrics-server $METRICS_SERVER_VERSION"
  METRICS_SERVER_VERSION=v0.4.1
  # curl -sLo metrics-server/components.yaml https://github.com/kubernetes-sigs/metrics-server/releases/download/${METRICS_SERVER_VERSION}/components.yaml
  kubectl apply -f metrics-server
}

metalLB() {
  METAL_VERSION=$(git ls-remote --tags --refs https://github.com/danderson/metallb.git | sort -t '/' -k 3 -V | tail -1 | awk -F/ '{print $3}')
  METAL_SECRET=$(openssl rand -base64 128 | xargs | sed -e 's/ //g')
  echo -e "\n MetalLB ${METAL_VERSION} \n"
  kubectl create namespace metallb-system
  kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey=$METAL_SECRET
  kubectl apply -f metalLB/metallb.yaml
  kubectl apply -f metalLB/km-config.yaml
  waitFor metallb-system
}

adminDashboard() {
  DASHBOARD_VERSION=$(
    git ls-remote --refs --tags https://github.com/kubernetes/dashboard.git | grep tags/v | sort -t '/' -k 3 -V | grep -v alpha | tail -1 | awk -F/ '{print $3}'
  )
  echo -e "\n Kubernetes Dashboard ${DASHBOARD_VERSION} \n"
  curl -so dashboard/kubernetes-dashboard.yaml https://raw.githubusercontent.com/kubernetes/dashboard/${DASHBOARD_VERSION}/aio/deploy/recommended.yaml
  kubectl apply -f dashboard/kubernetes-dashboard.yaml
  kubectl apply -f dashboard/admin-user-service-account.yaml
  kubectl apply -f dashboard/admin-user-secret.yaml
  waitFor kubernetes-dashboard
}

token() {
  [[ "$(kubectl get namespaces | grep kubernetes-dashboard)" == "" ]] && echo 'no token' && return
  kubectl describe secrets -n kubernetes-dashboard admin-user | grep ^token | awk '{print $2}'
}

createNetworkApps() {
  kubectl create namespace net-apps
  # kubectl -n net-apps apply -f backend/limitrange-limits.yaml
  kubectl apply -f backend/net-apps
  waitFor dns-nodejs && waitFor squid
}

createDatabase() {
  kubectl apply -f backend/mariadb && waitFor mariadb
  kubectl apply -f backend/postgres && waitFor postgres
  kubectl apply -f backend/mysql && waitFor mysql
  kubectl get all -A | grep -e mariadb -e mysql
}

k8sApps() {
  metricsServer
  adminDashboard
}

partnerContainer() {
  [[ $CLUSTER_NAME == production || $CLUSTER_NAME == staging ]] \
  && backend/partner/create-secret.sh \
  && {
    kubectl create namespace partner
    kubectl -n partner apply -f backend/limitrange-limits.yaml
    kubectl apply -n partner -f backend/partner && waitFor partner
  }
}

createJsx() {
  kubectl create namespace jsxjp
  kubectl -n jsxjp apply -f backend/limitrange-limits.yaml
  ingressTlsSecret jsxjp
  kubectl -n jsxjp apply -f backend/jsxjp && waitFor jsxjp
}

createMqtt() {
  kubectl create namespace mqtt
  kubectl -n mqtt apply -f backend/limitrange-limits.yaml
  ingressTlsSecret mqtt
  kubectl -n mqtt apply -f backend/mqtt && waitFor mqtt
}

ingressTlsSecret() {
  NAMESPACE=$1
  rm -fr tls && unzip /home/docker/partner/tls.zip -d tls
  kubectl -n $NAMESPACE create secret tls jsxjp-tls \
  --cert=tls/jsx.jp/fullchain.pem \
  --key=tls/jsx.jp/privkey.pem
}

ingressUpdateSecret() {
  NAMESPACE=$1
  kubectl -n $NAMESPACE create secret tls jsxjp-tls \
  --cert=tls/jsx.jp/fullchain.pem \
  --key=tls/jsx.jp/privkey.pem \
  --dry-run -o yaml | kubectl apply -f -
}

ingressNginxSimple() {
  kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml
}

ingressNginxSecure() {
  kubectl create namespace ingress-nginx
  kubectl -n ingress-nginx create configmap modsecurity-config \
  --from-file=custom-modsecurity.conf=ingress-nginx/conf/custom-modsecurity.conf
  kubectl apply -f ingress-nginx
}

ingressNginxAllow() {
  # default (Allow API)
  kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.5.1/deploy/static/provider/aws/deploy.yaml
}

ingressForward() {
  kubectl -n default apply -f backend/web && waitFor web
  kubectl -n default exec -it svc/web -- bash -c "curl -I -s jsxjp.jsxjp.svc.cluster.local:3000"
}

ingressRestart() {
  kubectl rollout restart deployment web
}

deployWordpress() {
  SQL="CREATE DATABASE IF NOT EXISTS wordpress;
CREATE USER IF NOT EXISTS 'wp-user'@'%' IDENTIFIED BY 'wp-admin';
GRANT ALL PRIVILEGES ON wordpress.* TO 'wp-user'@'%' WITH GRANT OPTION;
FLUSH PRIVILEGES;"

  kubectl exec -n mariadb -i svc/mariadb -- bash -c "echo \"$SQL\" | mariadb -pfalse"

  kubectl create namespace wordpress
  ingressTlsSecret wordpress
  kubectl -n wordpress apply -f backend/wordpress && waitFor wordpress
}

deployBitnami() {
  SQL="CREATE DATABASE IF NOT EXISTS wp_bitnami;
CREATE USER IF NOT EXISTS 'wp-user'@'%' IDENTIFIED BY 'wp-admin';
GRANT ALL PRIVILEGES ON wp_bitnami.* TO 'wp-user'@'%' WITH GRANT OPTION;
FLUSH PRIVILEGES;"

  kubectl exec -n mariadb -i svc/mariadb -- bash -c "echo \"$SQL\" | mariadb -pfalse"

  kubectl create namespace bitnami
  ingressTlsSecret bitnami
  kubectl -n bitnami apply -f backend/wp-bitnami && waitFor wp-bitnami
}

deployRedmine() {
  SQL="CREATE DATABASE IF NOT EXISTS redmine;
CREATE USER IF NOT EXISTS 'redmine-user'@'%' IDENTIFIED BY 'redmine-admin';
GRANT ALL PRIVILEGES ON redmine.* TO 'redmine-user'@'%' WITH GRANT OPTION;
FLUSH PRIVILEGES;"

  kubectl exec -n mysql -i svc/mysql -- bash -c "echo \"$SQL\" | mysql -pmy-secret-pw"

  kubectl create namespace redmine
  # kubectl -n redmine apply -f backend/limitrange-limits.yaml
  ingressTlsSecret redmine
  kubectl -n redmine apply -f backend/redmine && waitFor redmine
}

createStandard() {
  kubectl create namespace standard
  # kubectl -n standard apply -f backend/limitrange-limits.yaml
  # kubectl config set-context $(kubectl config current-context) --namespace standard
  ingressTlsSecret standard

  kubectl -n standard apply -f backend/redis && waitFor redis
  kubectl -n standard apply -f backend/mongo && waitFor mongo

  [[ $CLUSTER_NAME == production ]] && createApps

  kubectl describe nodes
  # kubectl config set-context $(kubectl config current-context) --namespace default
}

ingressBasicAuth() {
  kubectl -n $1 apply -f backend/basic-auth-secret.yaml
}

generateBasicAuth() {
  BASIC_CREDENTIALS=$(
    echo "${BASIC_SECRET:-bookworm}" | htpasswd -ni bookworm
  )
  kubectl create secret generic basic-auth \
  --from-literal=auth="${BASIC_CREDENTIALS}" \
  --dry-run=client -o yaml | kubectl -n $1 apply -f -
}

createReverse() {
  kubectl create namespace reverse
  kubectl -n reverse apply -f backend/limitrange-limits.yaml
  ingressTlsSecret reverse
  ingressBasicAuth reverse
  kubectl -n reverse apply -f backend/lo-stack && waitFor lo-stack
  kubectl -n reverse apply -f backend/www-lo && waitFor www-lo
  kubectl -n reverse apply -f backend/kde-lo && waitFor kde-lo
  kubectl -n reverse apply -f backend/code-lo && waitFor code-lo
  kubectl -n reverse apply -f backend/llama-lo && waitFor llama-lo
}

createCodeServer() {
  kubectl create namespace code-server
  ingressTlsSecret code-server
  ingressBasicAuth code-server
  kubectl -n code-server apply -f backend/code-server && waitFor code-server
}

createWetty() {
  if [[ true ]]
  then
    kubectl -n reverse apply -f backend/wetty-lo && waitFor wetty-lo
    return
  fi

  kubectl create namespace wetty
  # kubectl -n wetty apply -f backend/limitrange-limits.yaml
  ingressTlsSecret wetty
  kubectl -n wetty apply -f backend/wetty && waitFor wetty
  # TTY_PASSWD="$(makePasswd)"
  # genPasswd "$TTY_PASSWD"
  # echo "Initialized password: '$TTY_PASSWD'"
  # postSlack "Initialized password: '$TTY_PASSWD'"
}

makePasswd() {
  head -c 8 /dev/random | mkpasswd -s -5 | sed -e 's/\$//g' | sed -e 's/^1//'
}

genPasswd() {
  GEN_PASSWD="$1"
  [[ "$GEN_PASSWD" == "" ]] && echo -e "failed password.\nusage:\n\n  GEN_PASSWD='$(makePasswd)' genPasswd\n" && return
  kubectl -n wetty exec -i svc/wetty -- bash -c "echo bookworm:$GEN_PASSWD | chpasswd"
}

createCronJob() {
  [[ "$CLUSTER_NAME" != production ]] && return
  # kubectl create cronjob -n health-check value-domain --image ghcr.io/jobscale/value-domain --schedule='6/7 * * * *'
  kubectl create namespace health-check
  kubectl create cronjob -n health-check health-check --image ghcr.io/jobscale/health-check --schedule='8-55/11 * * * *'
  # kubectl create job -n health-check health-check-$(date +%s%3N) --from=cronjob.batch/health-check
  # kubectl apply -n health-check -f backend/cronjob/ec-cube-check.yaml
  # kubectl apply -n health-check -f backend/cronjob/ec-cube-healthy.yaml

  kubectl create namespace cron-job
  kubectl create cronjob -n cron-job remind --image ghcr.io/jobscale/remind --schedule='* * * * *'

  kubectl create cronjob -n cron-job kabuka --image ghcr.io/jobscale/kabuka --schedule='33 0,6 * * 1-5'
  ## Create One time job
  # kubectl create job -n cron-job kabuka-$(date +%s%3N) --from=cronjob.batch/kabuka

  kubectl create cronjob -n cron-job news-top --image ghcr.io/jobscale/news-top --schedule='9-55/11 * * * *'
  ## Create One time job
  # docker exec -it ${ENV:-production}-control-plane crictl pull ghcr.io/jobscale/news-top
  # kubectl create job -n cron-job news-top-$(date +%s%3N) --from=cronjob.batch/news-top

  kubectl create cronjob -n cron-job info-certificate --image ghcr.io/jobscale/info-certificate --schedule='48 23 * * *'
  ## Create One time job
  # docker exec -it ${ENV:-production}-control-plane crictl pull ghcr.io/jobscale/info-certificate
  # kubectl create job -n cron-job info-certificate-$(date +%s%3N) --from=cronjob.batch/info-certificate

  kubectl create cronjob -n cron-job speedtest --image ghcr.io/jobscale/speedtest --schedule '54 0,1,23 * * *'
  ## Create One time job
  # docker exec -it ${ENV:-production}-control-plane crictl pull ghcr.io/jobscale/speedtest
  # kubectl create job -n cron-job speedtest-$(date +%s%3N) --from=cronjob.batch/speedtest
}

createApps() {
  # deployNode
  kubectl -n standard apply -f backend/tanpo && waitFor tanpo
  kubectl -n standard apply -f backend/exp && waitFor exp
  {
    kubectl -n standard apply -f backend/todo && waitFor todo
    kubectl -n standard apply -f backend/sshwifty && waitFor sshwifty
    kubectl -n standard apply -f backend/zipcode && waitFor zipcode
    kubectl -n standard apply -f backend/mongo-blog && waitFor mongo-blog
    kubectl -n standard apply -f backend/simple-chat && waitFor simple-chat
  } &
  {
    # deployNginx
    kubectl -n standard apply -f backend/tetris && waitFor tetris
    kubectl -n standard apply -f backend/anywaychat && waitFor anywaychat
    kubectl -n standard apply -f backend/ramen-timer && waitFor ramen-timer
    # deployPhp
    kubectl -n standard apply -f backend/dokuwiki && waitFor dokuwiki
    kubectl -n standard apply -f backend/shop && waitFor shop
    # deployPython
    kubectl -n standard apply -f backend/tutorial && waitFor tutorial
    # kubectl -n standard apply -f backend/fastapi && waitFor fastapi
    # kubectl -n standard apply -f backend/django && waitFor django
    # deployEcho
    # kubectl -n standard apply -f backend/echo && waitFor echo
    kubectl -n standard apply -f backend/whoami && waitFor whoami
  } &

  wait
}

exposeLoadBalancer() {
  local NAME_SPACE=$1
  local NAME=$2
  local IMAGE=$3
  local PORT=$4
  local TARGET=$5
  local MIN_REPLICA=$6
  local MIN_REPLICA=1 # kubernetes.io/limit-ranger: LimitRanger plugin set: cpu, memory request for container
  kubectl create deployment -n $NAME_SPACE $NAME --image $IMAGE
  kubectl expose deployment -n $NAME_SPACE $NAME --name $NAME --type LoadBalancer --port $PORT --target-port $TARGET
  [[ "$MIN_REPLICA" != "0" ]] && kubectl autoscale deployment -n $NAME_SPACE $NAME $NAME \
  --cpu-percent 50 --min $MIN_REPLICA --max 20
  waitFor $NAME
}

exposeCluster() {
  local NAME_SPACE=$1
  local NAME=$2
  local IMAGE=$3
  local PORT=$4
  local TARGET=$5
  kubectl create deployment -n $NAME_SPACE $NAME --image $IMAGE
  kubectl expose deployment -n $NAME_SPACE $NAME --name $NAME --type ClusterIP --port $PORT --target-port $TARGET
  waitFor $NAME
}

postSlack() {
  DATA="{\"username\":\"Kubernetes\",\"text\":\"$1\"}"
  chSlack() {
    . installation > /dev/null 2>&1
    echo T$(chNote | base64 -d)TB
  }
  curl -s -X POST https://hooks.slack.com/services/$(chSlack) -d "$DATA" || echo
}

waitFor() {
  echo -n "        waiting" && loop.js 10 --silent
  for limit in {3..1}
  do
    [[ $(kubectl get pods -A | grep $1 | grep 1/1 | wc -l) > 0 ]] && echo -e "\n $1 - ok\n" && return
    echo " $1 - ng"
    echo -ne "just a moment please 6.."
    for i in {5..0}
    do
      delay
      [[ $(kubectl get pods -A | grep $1 | grep 1/1 | wc -l) > 0 ]] && echo -e "\n $1 - ok\n" && return
      delay
      [[ $(kubectl get pods -A | grep $1 | grep 1/1 | wc -l) > 0 ]] && echo -e "\n $1 - ok\n" && return
      echo -n " $i.."
    done
  done
}

delay() {
  sleep 8
}

loop() {
  loop.js $1 --silent
}

### unsupportd

delAll() {
  for app in laravel lumen blog simple-chat chat
  do
    kubectl delete svc $app
    kubectl delete deployment $app
  done
}

deployBacklog() {
  [[ $(grep ""vm.max_map_count /etc/sysctl.d/99-sysctl.conf | wc -l) < 1 ]] \
  && echo "vm.max_map_count = 262144" | sudo tee -a /etc/sysctl.d/99-sysctl.conf \
  && sudo sysctl --system
  kubectl run elasticsearch --env "http.host=0.0.0.0" --env "ES_JAVA_OPTS=-Xms512m -Xmx512m" --image docker.elastic.co/elasticsearch/elasticsearch-oss:6.8.2
  waitFor elasticsearch
  kubectl run graylog --env GRAYLOG_HTTP_EXTERNAL_URI="/" --port 9000 --port 12201 --port 1514 --port 5555 --image graylog/graylog:3.1
  kubectl expose deployment graylog --name graylog --type LoadBalancer--port 80 --target-port 9000
  waitFor graylog
}

portForward() {
  target_ns=standard
  target_svc=profile
  target_port=80
  bind_addr=0.0.0.0
  bind_port=3000
  node_port=$(kubectl get service -n ${target_ns} ${target_svc} -o=jsonpath="{.spec.ports[?(@.port==${target_port})].nodePort}")
  if [[ "$node_port" == "" ]]
  then
    kubectl get service -n ${target_ns} ${target_svc} -o wide
    echo -e "\ncan not get node port. check type LoadBalancer status.\n"
    kubectl port-forward -n ${target_ns} service/${target_svc} --address ${bind_addr} ${bind_port}:${target_port}
    return 1
  fi
  node_addr=$(docker network inspect kind | jp "[?Name=='kind']|[0].Containers.*|[?Name=='production-control-plane']|[0].IPv4Address" | sed -e 's/"//g' | awk -F/ '{print $1}')
  echo bind=$bind_addr:$bind_port node=$node_addr:$node_port
  docker run --name kind-proxy-${bind_port} -p ${bind_addr}:${bind_port}:${bind_port} --network kind \
    -d alpine/socat -dd tcp-listen:${bind_port},fork,reuseaddr tcp-connect:${node_addr}:${node_port}
}

defaultImage() {
  IMG_LIST=(
    ghcr.io/jobscale/wetty:dind
    docker.io/jobscale/dokuwiki
    ghcr.io/jobscale/anywaychat-web
    ghcr.io/jobscale/code-server
    ghcr.io/jobscale/dns-nodejs
    ghcr.io/jobscale/ec-cube
    ghcr.io/jobscale/health-check
    ghcr.io/jobscale/jsxjp
    ghcr.io/jobscale/log-upload
    ghcr.io/jobscale/mongo-blog
    ghcr.io/jobscale/mqtt-c-pub-sub
    ghcr.io/jobscale/news-top
    ghcr.io/jobscale/node-express-ejs-authorization
    ghcr.io/jobscale/ramen-timer
    ghcr.io/jobscale/remind
    ghcr.io/jobscale/reverse-proxy
    ghcr.io/jobscale/simple-chat
    ghcr.io/jobscale/squid
    ghcr.io/jobscale/sshwifty
    ghcr.io/jobscale/tanpo
    ghcr.io/jobscale/tetris
    ghcr.io/jobscale/todo-app
    ghcr.io/jobscale/whoami
    ghcr.io/jobscale/zipcode-jp
  )
  for i in ${IMG_LIST[@]}
  do
    echo -n "$i "
    time { loadImage $i; } | xargs echo -n
  done
}

loadImage() {
  docker pull $1
  kind load docker-image $1 --name $CLUSTER_NAME
  loop 3
}

pullCrictl() {
  echo "docker exec -i $CLUSTER_NAME-control-plane crictl pull $1"
  docker exec -di $CLUSTER_NAME-control-plane crictl pull $1
}

pullImages() {
  IMG_LIST=(
    $(grep image: backend/*/components.yaml | awk -F'image:' '{print $2}' | sort | uniq)
  )
  for i in ${IMG_LIST[@]}
  do
    echo -n "$i "
    time { docker exec -i $CLUSTER_NAME-control-plane crictl pull $i; } | xargs echo -n
  done
}
